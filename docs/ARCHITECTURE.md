# ğŸ—ï¸ Job Search Pipeline - Complete Architecture & Flow

## ğŸ“ Entry Points

The system has **2 main entry points**:

### 1. **CLI Entry Point** (`main.py`)
- **Purpose**: Command-line interface for running searches and viewing stats
- **Usage**: `python main.py [options]`
- **Modes**:
  - `--daily` - Run automated daily search
  - `--keywords` - Custom keyword search
  - `--stats` - View database statistics
  - `--skill-stats` - View skill frequency stats
  - `--pre-filter-stats` - View pre-filter statistics
  - `--usage-report` - View API usage costs
  - `--comprehensive` - Matrix search (keywords Ã— sites)
  - `--per-site` - Search each site individually

### 2. **Web API Entry Point** (`web_app.py`)
- **Purpose**: Flask web server for browser-based access
- **Usage**: `python web_app.py [--port 5000]`
- **Access**: `http://localhost:5000` (or network IP)
- **Endpoints**: REST API for viewing jobs, stats, running searches

---

## ğŸ”„ Complete Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MAIN PIPELINE FLOW                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START: main.py or web_app.py
  â”‚
  â”œâ”€â–¶ JobSearchPipeline.__init__()
  â”‚     â”‚
  â”‚     â”œâ”€â–¶ Config.validate()                    [config.py]
  â”‚     â”œâ”€â–¶ GoogleJobSearch()                     [search.py]
  â”‚     â”œâ”€â–¶ ContentExtractor()                    [extractor.py]
  â”‚     â”œâ”€â–¶ JobParser()                           [llm_parser.py]
  â”‚     â”œâ”€â–¶ JobDatabase()                         [storage.py]
  â”‚     â”œâ”€â–¶ JobFilter(USER_PROFILE)               [filters.py]
  â”‚     â””â”€â–¶ PreParseFilter(max_yoe)               [pre_filters.py]
  â”‚
  â””â”€â–¶ pipeline.run() or pipeline.run_daily()
        â”‚
        â”œâ”€â–¶ STEP 1: Google Search
        â”‚     â”‚
        â”‚     â””â”€â–¶ GoogleJobSearch.search_jobs()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ Builds Boolean queries: "AI engineer" site:greenhouse.io
        â”‚           â”œâ”€â–¶ Calls Google Custom Search API
        â”‚           â”œâ”€â–¶ Handles pagination (max 100 results)
        â”‚           â””â”€â–¶ Returns: List[{title, link, snippet, displayLink}]
        â”‚
        â”œâ”€â–¶ STEP 1.5: Early Filtering (Title/Snippet)
        â”‚     â”‚
        â”‚     â””â”€â–¶ JobFilter.should_skip_early()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ Checks excluded keywords (senior, staff, etc.)
        â”‚           â””â”€â–¶ Checks location (USA/Remote only)
        â”‚
        â”œâ”€â–¶ STEP 2: Content Extraction
        â”‚     â”‚
        â”‚     â””â”€â–¶ ContentExtractor.extract_batch()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ For each URL:
        â”‚           â”‚     â”‚
        â”‚           â”‚     â”œâ”€â–¶ Try Jina Reader API (for static sites)
        â”‚           â”‚     â”œâ”€â–¶ Try Playwright (for JS-heavy sites)
        â”‚           â”‚     â””â”€â–¶ Try BeautifulSoup (fallback)
        â”‚           â”‚
        â”‚           â””â”€â–¶ Returns: List[{url, content, success, method, error}]
        â”‚
        â”œâ”€â–¶ STEP 3: Pre-Parse Filtering (NEW - Cost Saving)
        â”‚     â”‚
        â”‚     â””â”€â–¶ PreParseFilter.filter_batch()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ Check YOE > max_yoe (regex)
        â”‚           â”œâ”€â–¶ Check non-US locations (regex)
        â”‚           â”œâ”€â–¶ Check citizenship/clearance (regex)
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ Saves filtered jobs â†’ pre_filtered_jobs table
        â”‚           â””â”€â–¶ Returns: (passed_contents, filtered_contents)
        â”‚
        â”œâ”€â–¶ STEP 4: LLM Parsing
        â”‚     â”‚
        â”‚     â””â”€â–¶ JobParser.parse_batch()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ For each extracted content:
        â”‚           â”‚     â”‚
        â”‚           â”‚     â”œâ”€â–¶ Call OpenAI GPT-4o-mini API
        â”‚           â”‚     â”œâ”€â–¶ Extract structured JSON:
        â”‚           â”‚     â”‚     - job_title, company, location
        â”‚           â”‚     â”‚     - yoe_required, required_skills
        â”‚           â”‚     â”‚     - nice_to_have_skills, salary, etc.
        â”‚           â”‚     â”‚
        â”‚           â”‚     â””â”€â–¶ Create ParsedJob object
        â”‚           â”‚
        â”‚           â””â”€â–¶ Returns: (List[ParsedJob], token_usage)
        â”‚
        â”œâ”€â–¶ STEP 4.5: Skill Tracking (NEW)
        â”‚     â”‚
        â”‚     â””â”€â–¶ JobDatabase.save_skill_frequencies()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ Normalize job title â†’ category
        â”‚           â”‚     (Data Scientist, ML Engineer, AI Engineer, etc.)
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ For each skill:
        â”‚           â”‚     INSERT/UPDATE skill_frequency table
        â”‚           â”‚     (skill_name, job_title_category, times_seen++)
        â”‚           â”‚
        â”‚           â””â”€â–¶ Tracks: Which skills appear in which job categories
        â”‚
        â”œâ”€â–¶ STEP 5: Relevance Filtering & Scoring
        â”‚     â”‚
        â”‚     â””â”€â–¶ JobFilter.filter_jobs()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ For each ParsedJob:
        â”‚           â”‚     â”‚
        â”‚           â”‚     â”œâ”€â–¶ Calculate relevance score (0-100):
        â”‚           â”‚     â”‚     - YOE match: +30 (or -50 if over max)
        â”‚           â”‚     â”‚     - Required skills: +5 each (max 25)
        â”‚           â”‚     â”‚     - Preferred skills: +3 each (max 15)
        â”‚           â”‚     â”‚     - Location match: +15
        â”‚           â”‚     â”‚     - Remote bonus: +5 to +10
        â”‚           â”‚     â”‚     - Title exclusion: -40
        â”‚           â”‚     â”‚
        â”‚           â”‚     â”œâ”€â–¶ Filter by min_score (default 30)
        â”‚           â”‚     â””â”€â–¶ Filter by location (USA/Remote only)
        â”‚           â”‚
        â”‚           â””â”€â–¶ Returns: List[(ParsedJob, score)] sorted by score
        â”‚
        â”œâ”€â–¶ STEP 6: Database Storage
        â”‚     â”‚
        â”‚     â””â”€â–¶ JobDatabase.save_batch()
        â”‚           â”‚
        â”‚           â”œâ”€â–¶ For each (job, score):
        â”‚           â”‚     â”‚
        â”‚           â”‚     â”œâ”€â–¶ INSERT OR IGNORE INTO jobs
        â”‚           â”‚     â”‚     (deduplication by URL)
        â”‚           â”‚     â”‚
        â”‚           â”‚     â””â”€â–¶ Track: saved vs skipped (duplicates)
        â”‚           â”‚
        â”‚           â””â”€â–¶ Returns: (saved_count, skipped_count)
        â”‚
        â”œâ”€â–¶ STEP 7: CSV Export
        â”‚     â”‚
        â”‚     â””â”€â–¶ JobDatabase.export_csv()
        â”‚           â”‚
        â”‚           â””â”€â–¶ Exports to: data/jobs_YYYYMMDD_HHMMSS.csv
        â”‚
        â””â”€â–¶ STEP 8: Resume Generation (Optional)
              â”‚
              â””â”€â–¶ ResumeGenerator.generate_resumes()
                    â”‚
                    â”œâ”€â–¶ For each new job:
                    â”‚     â”‚
                    â”‚     â”œâ”€â–¶ Match location â†’ approved resume location
                    â”‚     â”œâ”€â–¶ AI ranks projects by relevance
                    â”‚     â”œâ”€â–¶ Select top 3 projects
                    â”‚     â”‚
                    â”‚     â”œâ”€â–¶ select_skills_for_job() (NEW)
                    â”‚     â”‚     â”‚
                    â”‚     â”‚     â”œâ”€â–¶ Match JD skills â†’ extended_skills
                    â”‚     â”‚     â”œâ”€â–¶ Add relevant skills to resume
                    â”‚     â”‚     â””â”€â–¶ Track skills_added
                    â”‚     â”‚
                    â”‚     â”œâ”€â–¶ Generate LaTeX resume
                    â”‚     â”œâ”€â–¶ Compile to PDF (pdflatex)
                    â”‚     â”‚
                    â”‚     â””â”€â–¶ Save to:
                    â”‚           - resumes table
                    â”‚           - resume_changes table (NEW)
                    â”‚           - data/resumes/ directory
                    â”‚
                    â””â”€â–¶ Returns: List[{success, tex_path, pdf_path, skills_added}]

END: Summary dict with statistics
```

---

## ğŸ”— Component Dependencies

### **Core Components (Connected)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CORE PIPELINE CHAIN                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

main.py / web_app.py
  â”‚
  â””â”€â–¶ JobSearchPipeline (pipeline.py)
        â”‚
        â”œâ”€â–¶ GoogleJobSearch (search.py)
        â”‚     â””â”€â–¶ Depends on: config.py (API keys)
        â”‚
        â”œâ”€â–¶ ContentExtractor (extractor.py)
        â”‚     â”œâ”€â–¶ Uses: Jina Reader API
        â”‚     â”œâ”€â–¶ Uses: Playwright (browser automation)
        â”‚     â””â”€â–¶ Uses: BeautifulSoup (HTML parsing)
        â”‚
        â”œâ”€â–¶ PreParseFilter (pre_filters.py) [NEW]
        â”‚     â””â”€â–¶ Independent: Pure regex, no external deps
        â”‚
        â”œâ”€â–¶ JobParser (llm_parser.py)
        â”‚     â””â”€â–¶ Depends on: OpenAI API (GPT-4o-mini)
        â”‚
        â”œâ”€â–¶ JobFilter (filters.py)
        â”‚     â””â”€â–¶ Depends on: USER_PROFILE (config.py)
        â”‚
        â””â”€â–¶ JobDatabase (storage.py)
              â””â”€â–¶ SQLite database (persistent storage)
```

### **Independent Components**

These can be used standalone or in different contexts:

1. **`config.py`**
   - **Independent**: No dependencies on other modules
   - **Used by**: All components (singleton pattern)
   - **Purpose**: Centralized configuration management

2. **`storage.py` (JobDatabase)**
   - **Independent**: Can be used without pipeline
   - **Dependencies**: SQLite, json
   - **Can be used for**: Direct database queries, stats, exports

3. **`filters.py` (JobFilter)**
   - **Independent**: Can filter any list of ParsedJob objects
   - **Dependencies**: USER_PROFILE from config
   - **Can be used for**: Standalone job scoring

4. **`pre_filters.py` (PreParseFilter)** [NEW]
   - **Independent**: Pure regex filtering, no external APIs
   - **Dependencies**: None (except re, logging)
   - **Can be used for**: Pre-filtering any text content

5. **`resume_generator.py` (ResumeGenerator)**
   - **Semi-independent**: Can generate resumes from job data
   - **Dependencies**: OpenAI API, LaTeX compiler, YAML config
   - **Can be used for**: Standalone resume generation

---

## ğŸ“Š Data Flow

### **Data Structures**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA TRANSFORMATIONS                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Search Results (Dict)
   {
     "title": "AI Engineer",
     "link": "https://...",
     "snippet": "...",
     "displayLink": "greenhouse.io"
   }
   â”‚
   â””â”€â–¶ [Early Filter] â†’ Filtered by title/snippet
   â”‚
   â””â”€â–¶ [Extraction] â†’ Extracted Content (Dict)
         {
           "url": "https://...",
           "content": "Full job posting text...",
           "success": True,
           "method": "jina" | "playwright" | "beautifulsoup"
         }
         â”‚
         â””â”€â–¶ [Pre-Filter] â†’ Filtered by regex (YOE, location, citizenship)
         â”‚
         â””â”€â–¶ [LLM Parse] â†’ ParsedJob (Pydantic Model)
               {
                 job_title: str
                 company: str
                 location: str
                 yoe_required: int
                 required_skills: List[str]
                 nice_to_have_skills: List[str]
                 ...
               }
               â”‚
               â””â”€â–¶ [Skill Tracking] â†’ skill_frequency table
               â”‚
               â””â”€â–¶ [Scoring] â†’ (ParsedJob, score: int)
                     â”‚
                     â””â”€â–¶ [Storage] â†’ jobs table (SQLite)
                           â”‚
                           â””â”€â–¶ [Resume Gen] â†’ LaTeX + PDF
                                 â”‚
                                 â””â”€â–¶ resumes table + resume_changes table
```

---

## ğŸ—„ï¸ Database Schema

### **Tables & Relationships**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATABASE STRUCTURE                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

jobs (Main table)
  â”œâ”€ id (PK)
  â”œâ”€ url (UNIQUE)
  â”œâ”€ title, company, location
  â”œâ”€ yoe_required, required_skills, nice_to_have_skills
  â”œâ”€ relevance_score
  â””â”€ created_at, updated_at

resumes
  â”œâ”€ id (PK)
  â”œâ”€ job_id (FK â†’ jobs.id)
  â”œâ”€ job_title, company, job_url
  â”œâ”€ resume_location, selected_projects
  â”œâ”€ tex_path, pdf_path
  â””â”€ created_at

resume_changes [NEW]
  â”œâ”€ id (PK)
  â”œâ”€ resume_id (FK â†’ resumes.id)
  â”œâ”€ job_id (FK â†’ jobs.id)
  â”œâ”€ location_used
  â”œâ”€ skills_added (JSON)
  â”œâ”€ projects_selected (JSON)
  â””â”€ created_at

pre_filtered_jobs [NEW]
  â”œâ”€ id (PK)
  â”œâ”€ url (UNIQUE)
  â”œâ”€ title, snippet, source_domain
  â”œâ”€ filter_reason, filter_details
  â”œâ”€ raw_content_preview
  â””â”€ created_at

skill_frequency [NEW]
  â”œâ”€ id (PK)
  â”œâ”€ skill_name
  â”œâ”€ job_title_category
  â”œâ”€ times_seen
  â”œâ”€ first_seen, last_seen
  â””â”€ UNIQUE(skill_name, job_title_category)

unextracted_jobs
  â”œâ”€ id (PK)
  â”œâ”€ url (UNIQUE)
  â”œâ”€ title, snippet, source_domain
  â”œâ”€ extraction_methods_attempted (JSON)
  â”œâ”€ error_message
  â”œâ”€ retry_count
  â””â”€ created_at, updated_at
```

---

## ğŸ”Œ External Dependencies

### **API Services**

1. **Google Custom Search API**
   - Used by: `search.py`
   - Cost: Free tier (100 queries/day), then $5/1,000 queries
   - Rate limit: 100 queries/day (free tier)

2. **Jina Reader API**
   - Used by: `extractor.py`
   - Purpose: Extract content from static websites
   - Cost: Free tier available

3. **OpenAI API (GPT-4o-mini)**
   - Used by: `llm_parser.py`, `resume_generator.py`
   - Cost: ~$0.50 per 1,000 job postings parsed
   - Rate limit: Based on tier

4. **Playwright**
   - Used by: `extractor.py`
   - Purpose: Browser automation for JS-heavy sites
   - Local dependency (no API calls)

---

## ğŸ¯ Key Design Patterns

### **1. Pipeline Pattern**
- Sequential processing through stages
- Each stage transforms data for the next
- Failures at any stage don't crash the pipeline

### **2. Strategy Pattern**
- Multiple extraction methods (Jina, Playwright, BeautifulSoup)
- Fallback chain: Try method 1, if fails try method 2, etc.

### **3. Repository Pattern**
- `JobDatabase` abstracts database operations
- All database access goes through storage layer

### **4. Configuration Singleton**
- `config.py` provides centralized configuration
- Single source of truth for API keys, paths, settings

### **5. Dependency Injection**
- Components initialized in `JobSearchPipeline.__init__()`
- Can be swapped/mocked for testing

---

## ğŸ”„ Alternative Flows

### **Web App Flow**

```
User Browser
  â”‚
  â””â”€â–¶ web_app.py (Flask)
        â”‚
        â”œâ”€â–¶ GET /api/jobs â†’ JobDatabase.get_jobs()
        â”œâ”€â–¶ GET /api/stats â†’ JobDatabase.get_stats()
        â”œâ”€â–¶ GET /api/skills â†’ JobDatabase.get_top_skills_by_category()
        â”œâ”€â–¶ GET /api/pre-filtered â†’ JobDatabase.get_pre_filtered_jobs()
        â””â”€â–¶ POST /api/search/run â†’ JobSearchPipeline.run()
```

### **Resume Generation Flow**

```
generate_resumes.py (standalone script)
  â”‚
  â””â”€â–¶ ResumeGenerator.generate_recommendations()
        â”‚
        â”œâ”€â–¶ Load jobs from database
        â”œâ”€â–¶ Rank projects using AI
        â”œâ”€â–¶ Select skills dynamically
        â””â”€â–¶ Generate LaTeX â†’ PDF
```

### **Stats/Reporting Flow**

```
main.py --stats
  â”‚
  â””â”€â–¶ JobDatabase.get_stats()
        â”‚
        â”œâ”€â–¶ Count jobs, applied, saved
        â”œâ”€â–¶ Top companies, domains
        â””â”€â–¶ Average YOE

main.py --skill-stats
  â”‚
  â””â”€â–¶ JobDatabase.get_skill_stats_summary()
        â”‚
        â”œâ”€â–¶ Unique skills count
        â”œâ”€â–¶ Skills by category
        â””â”€â–¶ Top skills by frequency
```

---

## ğŸš¦ Error Handling & Recovery

### **Retry Logic**

1. **Google Search API** (`search.py`)
   - Retries: 3 attempts with exponential backoff
   - Handles: HttpError, ConnectionError

2. **Content Extraction** (`extractor.py`)
   - Fallback chain: Jina â†’ Playwright â†’ BeautifulSoup
   - Failed extractions saved to `unextracted_jobs` table

3. **LLM Parsing** (`llm_parser.py`)
   - Retries: 3 attempts with exponential backoff
   - Failed parses logged but don't stop pipeline

### **Failure Points**

- **Search fails**: Pipeline stops (no URLs to process)
- **Extraction fails**: Job saved to `unextracted_jobs` for retry
- **Parsing fails**: Job skipped, logged
- **Database fails**: Transaction rolled back, error logged

---

## ğŸ“ˆ Performance Considerations

### **Bottlenecks**

1. **LLM Parsing** (slowest)
   - Cost: ~$0.50 per 1,000 jobs
   - Time: ~2-5 seconds per job
   - **Solution**: Pre-filtering reduces LLM calls by 20-40%

2. **Content Extraction**
   - Playwright: ~3-5 seconds per page
   - Jina: ~1-2 seconds per page
   - **Solution**: Batch processing, parallel extraction

3. **Google Search API**
   - Rate limit: 100 queries/day (free tier)
   - **Solution**: Comprehensive search uses pagination efficiently

### **Optimizations**

- Pre-filtering saves 20-40% of LLM costs
- Early filtering (title/snippet) saves extraction costs
- Batch database operations (save_batch)
- Caching: Deduplication by URL prevents reprocessing

---

## ğŸ” Security & Privacy

- **API Keys**: Stored in `.env` file (not committed)
- **Database**: Local SQLite (no network exposure)
- **Web App**: Runs on localhost by default (can bind to network)
- **Data**: Job postings are public data, no PII stored

---

## ğŸ“ Summary

**Entry Points**: `main.py` (CLI), `web_app.py` (Web)

**Main Flow**: Search â†’ Extract â†’ Pre-Filter â†’ Parse â†’ Track Skills â†’ Score â†’ Store â†’ Generate Resumes

**Connected Components**: Pipeline orchestrates all components sequentially

**Independent Components**: Config, Storage, Filters, Pre-Filters can be used standalone

**Data Flow**: Dict â†’ Dict â†’ ParsedJob â†’ (ParsedJob, score) â†’ Database â†’ LaTeX/PDF

**Key Innovation**: Pre-filtering reduces LLM costs by filtering disqualifying jobs before expensive API calls
